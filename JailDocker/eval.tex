\section{Evaluation}

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 1: 실험 환경 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
In this section we discuss the docker container-based partitioning on the
scale-up server described in Section 3.
We ran the four benchmarks on Linux 4.5-rc4 with stock Linux. 
All experiments were performed on a 120 core machine with 8-socket, 15-core
Intel E7-8870 chips equipped with 792 GB DDR3 DRAM.
We used ram file system for HDFS due to the eliminating the HDFS bottlenect.
\else

\fi




%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 2: 비교 대상 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
%기본으로 4G의 메모리를 heap 메모리 사이즈로 설정하였고, input 데이터는 10G 이상으로 설정하였다.
We used four different experiment settings.
First, we used non-partitioning method as Figure~ section 2 graph and we set
heap size(4G).
Second we used fine-grained partitiong that is per-socket(15 core) partitiong
because it can make maximize locality.
We allocated heap size by modifing heap size is that we divide 4G of number of
partitining.
Finaly, we used corese-grained partitiong that is per-socket(30 core) partitiong
since it can mitegrate the straggler tasks problem.
\else

\fi


%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 2: WC, NB 결과 설명 
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
The results for Word Count are shown in Figure~\ref{fig:docker}(a), and the
result shows the thoughput of BigDataBench with our four different settings.
Up to 60 core, the PS GC version of non-partiting approach scales linearly and
then it flattens out.
However, up to 60 core, our per-socket partitioning outperform non-partitioning
since it can remove GC and NUMA latency overheads, and then the straggler tasks
problem become bottlenecks.
our corese-grained partitiong outperforms non-partitioning by 1.5x and
per-socket partiting by 1.5x on 120 core.
The results(Figure~\ref{docker}(b)) for Naive Basian is similar to Word Count
workload.
Our per-socket partitining outperform non-partitioning by 1.5x and per-socket
partidfa by 1.5. on 120 core.
\else 

\fi


%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 3: Grep 결과 설명 
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
The results for Word Count are shown in Figure~\ref{fig:docker}(a), and the
result shows the thoughput of BigDataBench with our four different settings.
Up to 60 core, the PS GC version of non-partiting approach scales linearly and
then it flattens out.
\else 

\fi


%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
% Paragraph 4: K-means 결과 설명 
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
The results for Word Count are shown in Figure~\ref{fig:docker}(a), and the
result shows the thoughput of BigDataBench with our four different settings.
Up to 60 core, the PS GC version of non-partiting approach scales linearly and
then it flattens out.
However, up to 60 core, our per-socket partitioning outperform non-partitioning
since it can remove GC and NUMA latency overheads, and then the straggler tasks
problem become bottlenecks.
our corese-grained partitiong outperforms non-partitioning by 1.5x and
per-socket partiting by 1.5x on 120 core.
\else 

\fi

