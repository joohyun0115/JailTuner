%Introduce
Good afternoon, Ladies and gentlemen.
It's a great pleasure to be with you today.
My name's Joohyun Kyong and I'm a Phd Student at Kookmin University.

This afternoon I'd like to talk about 'Improving Scalability of Apache
Spark-based Scale-up Server through Docker Container-based Partitioning'.
My talk will last about 10 minutes.

%Outline
What I intend to do is to break down this presentation into four parts:
First, I'll describe the history of the Apache Spark Scalability, reviewing some
of the early studies and their technical issues.
Then I'll talk about what is Spark scalability problem.
In particular, I'll describe the problem from an academic point of you.
Then I'll show you our new concept, 
so I'll share with you some of the improved result.
Finally, I'll make a new architecure and talk about future plan.

There's quite a lot to cover, so I'd be grateful if you'd hold any questions
until the end of my talk.

%Microprocessor Trend Data
I'll describe some of the background research.
Previous research show two interesting charicterastic.

First, increasing Big Data Market. 
Every 60 seconds,  98,000 tweets 
and about 2,000 data create and they were connected to the Internet.
Second, 40 years of microprocessor trend data show interesting charicterastic.
The number of cores is now increasing up to 200 core.
Therefore, number of CPU and Data are growth.

Scale-out and scale-up server are two different methods to big data frameworks.
Scale-out server upgrades are performed through adding nodes to the existing
cluster system.
On the other hand, scale-up server upgrades are adding resources(for example,
CPU, memory) to the existing single node system.

However, Until recently, big Data frameworks focus has been on Scale-out such
as Hadoop and Spark frameworks.

NSDI, SIGCOMM, ASPLOS OSDI, SOSP papers are focus has been on Scale-out environment.

%Let me refresh your memory on the history of the Apache Spark Scalability
%research, published by Dr Wickizer at the MIT University in 2010s.

But, what about scale-up server? 
Why scale-up when you can scale-out?
Not only the number of cores is now increasing, but also sometimes
shared-memory multicore system is order of magnitude better performance.
For example, GraphLab1 (multicore) is 1000x faster than Hadoop, and scale-up
servers are mostly used in scientific analytics areas.

We focused on Apache Spark and we used a 120-core (8 socket and each socket have 15 cores) that 
Intel Xeon process and we used BigDataBench.
BigDataBench is composed of various workload such as Word Count, Naive
Basion, Grep and K-means.  
This page shows our configurations, we used four workloads.
For the simplicity of experiment, we used input data size as the this one.

This figure shows the Apache Spark scalability problem.
Here, y axis shows the throughput and the x axis shows the number of cpu.
The hight y line shows the better performance.
We ran on 120 core system with tmpfs filesystems to minimize IO bottlenecks.
Up to 30 core, the system scales up linearly, but then they flattens out.
Idle scale-up server shows linear scalability.
However, real scale-up server scales flattens out.
This four workloads show similar results. 

Indeed, it is  the general scalability problems.
It means that garbage collection(GC) overheads and locality of memory accesses are scale-up server scalability
problems by previous research.

Spark and Hadoop frameworks run on Java virtual machine.
To preliminary evaluate the NUMA partitioning effect, we used two different settings.
First, all threads are scheduled by the OS to migrate any core, and we enabled
automatic NUMA balancing feature in the Linux kernel.
Second, we used per-socker JVM partitioning by using the NUMA control. 
The result show that partitioning approach has many advantages over
non-partitioning approach.

%Container-based Partitioning 
So, for now, we focus on our Docker container-based partitioning approach.
The results for Word Count are shown in this figure.
Up to 60 core, the non-partitioning approach scales linearly and then it flattens out.
However, up to 60 core, our fine-grained partitioning that is
per-socket partitioning outperforms non-partitioning 
because it can remove GC and NUMA latency overheads.
And then a straggler tasks problem become bottlenecks.
Our coarse-grained partitioning 
that each partitioning have 30 cores outperforms
non-partitioning and fine-grained partitioning.

The results for K-means are shown in this figure,
The K-means workload suffers from GC therefore, fine-grained partitioning
approach has substantial performance scalability up to 60 core.
However, then it collapses since it extremely suffers from the
straggler tasks problem that extends job completion times.
Our coarse-grained partitioning outperforms non-partitioning.

%Design Consideration
As noted earlier, the major problems of Spark scalability are GC overhead and
remote memory access.
We must consider carefully design to reduce the these problems.
For example, finding best-fit CPU counts
and solving the straggler tasks(i.e, tasks take significantly longer than
expected to complete) problem and Improving the NUMA locality and Avoiding
operating systems noise.


%The main components of the framwork are the decistion engine.
This figure our vision that will accommodate the previous
mentioned design consideration.
The left side of figure shows our proposed architecture, and the right side of
figure shows isolated Docker containers and per-socket CPU with memory.

Decision engine is one of the most important features
since all partitioning policy come from the decision engine component.
The basic function of the decision engine chooses whether or not the job
run on the Docker container.
The necessity of the auto-tuner is that performance scalability depending on
partitioning size commonly differs from each server architecture.
To maximized CPU utilization, the straggler monitor and core injector are needed
because straggler tasks prolong job completion times, so the
early finished CPUs should inject to other Docker containers, which
contains the straggler tasks.

%Summary
I sincerely hope you'll all go away with a more complete picture of the 
Apache scalability issue on the scale-up server.
Let me just run over the key points again and just summarize the main issues as
I see them.

Docker container-based partitioning method for Apache Spark scalability can
eliminate Garbage Collection overheads and remote memory access by divided
per-socket and best-fit partitioning.
Our evalution results show our Docker-container method has nice performance.

%Future Directions.
Our future directions are implementing the proof-of-concept architecture. 
This paper shows manually partitioning method, so we will implement the
Docker-container-based partitioning method.
and solving the straggler tasks problem. 
straggler tasks significantly extend job completion times.
To mitigate this problem, we may use dynamic resource allocation solution in
Docker to maximized CPU utilization.
\end{itemize}

That brings me to the end of my presentation.
Thank you for listening.

I'd be glad to try and answer any questions.

Questions:
