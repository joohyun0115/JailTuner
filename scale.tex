
\section{Scale-up Server Scalability}
\label{sec:scale}

\subsection{Test-bed and Benchmark}

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
% Apache Spark에 대한 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
\noindent
\textbf{Apache Spark. }
Apache Spark is a framework for large scale distributed computation.
RDD(Resilient Distributed Datasets) is a collection of partitions of records, 
and the RDD is managed as LRU(Least Recently Used), so when there is not enough
memory, Spark evicts the least recently used a partition from RDD.

%\begin{figure}[h]
%    \centering
%    \begin{subfigure}[b]{0.22\textwidth}
%        \includegraphics[width=1.5in]{graph/wc_max.eps}
%        \caption{Word Count}
%    \end{subfigure}%
%    \begin{subfigure}[b]{0.22\textwidth}
%        \includegraphics[width=1.5in]{graph/grep_max.eps}
%        \caption{Grep}
%    \end{subfigure}
%    \caption{CPU utilization on 120 core.}
%    \label{fig:utilization}
%\end{figure}

%Figure shows a substantial performance scalability of Spark when dataset can
% fit in memory(heap size > data size).
%However, large scale data(head size < data size) does not scale on scale-up
%server due to the GC and the memory latency.
\else
\fi

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
% 테스트 베드 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
\noindent
\textbf{Test-bed. }
We used a machine to evaluate on real hardware: an 120-core (8 sockets × 15
cores) Intel Xeon E7-8870 (the same machine used for evaluation in section~\ref{sec:eval})
and, to show that our conclusions generalize.
Hyper-Threading was disabled, and we used Linux kernel 4.5-rc6.


\begin{figure}[h]
  \begin{center}
     \includegraphics[width=0.3\textwidth]{fig/xeon}
  \end{center}
  \caption{Test-bed intel xeon archtecture.}
  \label{fig:basic}
\end{figure}
\else

\fi

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Benchamrk에 대한 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
\noindent
\textbf{Benchmark.} We used the BigDataBench.

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{l c c c c} \toprule
    Workload & Input data size & Heap size & Configuration & Data type\\
    \midrule
    Word Count & 10G & 4G & none & text \\ 
    Naive Basian & 10G & 4G & none & text\\
    Grep & 30G & 4G & \code{"the"} & text\\
    K-means & 4G & 4G & k=8 & graph\\
    \bottomrule
  \end{tabular}
  \begin{tabular}{l l l l l} \toprule
    JVM & Spark & Hadoop & OS & Distribution\\
    \midrule
    Openjdk 1.8.0\_91 & 1.3.1 & 1.2.1 & Linux 4.5-rc6 & Ubuntu 14.04\\ 
    \bottomrule
  \end{tabular}
  \caption{System information and configuration values.}
  \label{tab:memuse}
\end{table}

Table~\ref{tab:memuse} shows our configurations. we used four
workloads(Word Count, Naive Basian, Grep and K-means).
For the simplicity of experiment, we used input data as the table~\ref{tab:memuse}.
Of course, large Spark heap size can eliminate the GC overhead, but commonly the
input data size is larger than the heap size in big data analytics area; we
used the smaller heap size than the input data size.

\else

\fi

\subsection{Spark Scalability Problem}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Scalability 결과에 대한 대략 적인 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Figure~\ref{fig:scalability} shows the Spark scalability of four workloads with
two state of the art garbage collections, G1 and Parallel Scavenge(PS).
Up to 60 core, the four workloads scale lineally and then GC pause becomes bottlenecks.
The Word Count workload flattens out after 60 core, and other benchmarks slightly go
down because not only the GC overhead but also the remote memory access. 
To evaluate state of the art GC, we compared the G1 with PS GC.
The effect of changing to the GC is the PS outperforms G1 up to 2.0x on 120 core.
Although we used the state of the art scalable GC,
the Spark performance scalability still suffers from GC and NUMA locality problems, and
we could not see any significant differences when increasing the size
of executors.

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%CPU utilization에 대한 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Our goal is to maximize CPU utilization, so we profiled the CPU utilization
on the four workloads.
Figure ~\ref{fig:cpuutilization} shows the CPU utilizations.
Th y-axis is the percentage of time spent in kernel-space code(sys), user-space
code(user), and idle time(idle).
All benchmarks increase the idle time due to the GC pause as core counts increase.

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
% Linux kernel scalability (lock, cache cohearnci, scheduler)등등 OS 노이즈에 대한 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\ifkor
%NUMA의 영향 뿐만 아니라, 추가적으로 operating system의 scalability 저해 요소 때문에 
%파티션닝 방법은 필요하다.
%우리는 operation system에서 scalability의 영향을 주는 것을 확인하기 위해 가장먼저
%lock을 조사해보았다.
%첫째로 공유 데이터를 lock이 있다. 표 xxx 앞에서 실험한 spark의 wordcount에 대해서 .
%JVM 위에서 동작하는 thread간의 공유하는 single address space때문에 발생하는 공유 문제이다.
%다음으로 scheduler가 아직 
%마지막으로 cache cohearci traffic이 있다. 
\else

\fi



\subsection{Benefit of JVM Partitioning}
%\subsection{NUMA}

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%NUMA 영향에 대한 대략 적인 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

Spark and Hadoop frameworks use JAVA, and they need java virtual machine(JVM), so
understanding the JVM partitioning is important.
To preliminarily analyse the JVM partitioning effect, we conducted
benchmarking by using SPECjbb2013~\cite{Pogue2014SO}, which is a state of the art
benchmark for JVM performance.
We used two different experimental settings. 
First, we used per-socket JVM partitioning by using the NUMA control application(numactl).
Second, we set maximum JVM heap size, an available system
memory size, and threads are scheduled by the OS to
migrate any core, and we enable automatic NUMA
balancing feature in the Linux kernel.

\begin{figure}[h]
  \begin{center}
     \includegraphics[width=0.4\textwidth]{graph/SPECjbb2013}
  \end{center}
  \caption{Effect on JVM partitioning.}
  \label{fig:SPECJBB}
\end{figure}

The results shows that partitioning approach outperforms non-partitioning approach
by 1.4x on 120 core(figure~\ref{fig:SPECJBB}).
Therefore, in manycore scale-up server, partitioning approach has many
advantages over non-partitioning approach in terms of performance scalability.

